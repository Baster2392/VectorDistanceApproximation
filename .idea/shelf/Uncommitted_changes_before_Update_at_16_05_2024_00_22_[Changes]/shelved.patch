Index: vector_generator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy\r\nimport numpy as np\r\nimport torch\r\n\r\n\r\ndef generate_vector(min_value, max_value, size, for_recurrent=False):\r\n    if not for_recurrent:\r\n        vector = numpy.array([abs(np.random.randn()) * (max_value - min_value) + min_value for _ in range(size)])\r\n    else:\r\n        vector = numpy.array([[abs(np.random.randn()) * (max_value - min_value) + min_value] for _ in range(size)])\r\n    return vector\r\n\r\n\r\ndef generate_vector_pair(min_value, max_value, vector_size, for_recurrent=False):\r\n    if not for_recurrent:\r\n        vector_pair = np.zeros((2, vector_size))\r\n        vector_pair[0] = generate_vector(min_value, max_value, vector_size)\r\n        vector_pair[1] = generate_vector(min_value, max_value, vector_size)\r\n    else:\r\n        vector_pair = np.zeros((2, vector_size, 1))\r\n        vector_pair[0] = generate_vector(min_value, max_value, vector_size, for_recurrent=True)\r\n        vector_pair[1] = generate_vector(min_value, max_value, vector_size, for_recurrent=True)\r\n    return vector_pair\r\n\r\n\r\ndef generate_vector_pairs(min_value, max_value, vector_size, pairs_number, for_recurrent=False):\r\n    if not for_recurrent:\r\n        vector_pairs = np.zeros((pairs_number, 2, vector_size))\r\n        for i in range(pairs_number):\r\n            vector_pairs[i] = generate_vector_pair(min_value, max_value, vector_size)\r\n    else:\r\n        vector_pairs = np.zeros((pairs_number, 2, vector_size, 1))\r\n        for i in range(pairs_number):\r\n            vector_pairs[i] = generate_vector_pair(min_value, max_value, vector_size, for_recurrent=True)\r\n    return vector_pairs\r\n\r\n\r\ndef calculate_distance(vector1, vector2):\r\n    return np.linalg.norm(vector1 - vector2)\r\n\r\n\r\ndef generate_sample_data(number_of_samples, min_value, max_value, vector_size, split_pairs=False):\r\n    if not split_pairs:\r\n        sample_pairs = generate_vector_pairs(min_value, max_value, vector_size, number_of_samples)\r\n        sample_distances = np.zeros((number_of_samples, 1))\r\n        for i in range(number_of_samples):\r\n            sample_distances[i] = calculate_distance(sample_pairs[i][0], sample_pairs[i][1])\r\n        return sample_pairs, sample_distances\r\n    else:\r\n        lefts, rights, distances = [], [], []\r\n        for i in range(number_of_samples):\r\n            lefts.append(generate_vector_pair(min_value, max_value, vector_size))\r\n            rights.append(generate_vector_pair(min_value, max_value, vector_size))\r\n            distances.append(calculate_distance(lefts[i], rights[i]))\r\n        return lefts, rights, distances\r\n\r\n\r\ndef generate_sample_data_for_recurrent(number_of_samples, min_value, max_value, vector_size):\r\n    sample_pairs = generate_vector_pairs(min_value, max_value, vector_size, number_of_samples, for_recurrent=True)\r\n    sample_distances = np.zeros((number_of_samples, 1))\r\n    for i in range(number_of_samples):\r\n        sample_distances[i] = calculate_distance(sample_pairs[i][0], sample_pairs[i][1])\r\n    return sample_pairs, sample_distances\r\n\r\n\r\n# tests\r\nif __name__ == '__main__':\r\n    x_data, y_data = generate_sample_data_for_recurrent(10, 0, 1, 3)\r\n    print(x_data)\r\n    print(y_data)\r\n\r\n
===================================================================
diff --git a/vector_generator.py b/vector_generator.py
--- a/vector_generator.py	
+++ b/vector_generator.py	
@@ -35,8 +35,8 @@
     return vector_pairs
 
 
-def calculate_distance(vector1, vector2):
-    return np.linalg.norm(vector1 - vector2)
+def calculate_distance(a, b):
+    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
 
 
 def generate_sample_data(number_of_samples, min_value, max_value, vector_size, split_pairs=False):
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport csv\r\nimport itertools\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\n\r\nfrom models.siamese_model_no_norm import SiameseNetworkNoNorm\r\nimport vector_generator as vg\r\n\r\nCSV_FILE_PATH = 'results/res2.csv'\r\n\r\n\r\ndef validate(model, criterion, input_dim):\r\n    model.eval()\r\n    val_loss = 0.0\r\n    batch_size = 32\r\n\r\n    with torch.no_grad():\r\n        x_validate, y_validate = vg.generate_sample_data(batch_size, 0, 1, input_dim)\r\n        outputs = model(x_validate)\r\n        loss = criterion(outputs, y_validate)\r\n        val_loss = loss.item()\r\n\r\n    return val_loss\r\n\r\n\r\ndef training(input_dim, hidden_dim, learning_rate, num_layers, patience=600):\r\n    n_samples = 32\r\n    max_value = 1\r\n    min_lr = 1e-8\r\n    patience_after_min_lr = 1000\r\n    loops_after_min_lr = 0\r\n    factor = 0.75\r\n\r\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n    model = SiameseNetworkNoNorm(input_dim, hidden_dim, num_layers).to(device)\r\n    criterion = nn.L1Loss().to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\r\n    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=patience, factor=factor, verbose=True, min_lr=min_lr)\r\n\r\n    # Training loop\r\n    epochs = 25000\r\n    for epoch in range(epochs):\r\n        x_train, y_train = vg.generate_sample_data(n_samples, 0, max_value, input_dim, False)\r\n        x_train = torch.tensor(x_train, dtype=torch.float).to(device)\r\n        y_train = torch.tensor(y_train, dtype=torch.float).to(device)\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(x_train)\r\n        loss = criterion(outputs, y_train)\r\n        loss.backward()\r\n        optimizer.step()\r\n        scheduler.step(loss.item())\r\n        # best to keep commented when using grid search\r\n        # Print loss for each epoch\r\n        if epoch % 10 == 0:\r\n            print(\r\n                f'Epoch [{epoch}/{epochs}], Id: {input_dim} Loss: {loss.item()}, lr={optimizer.param_groups[0][\"lr\"]}')\r\n\r\n        if loss.item() < 0.05:\r\n            break\r\n\r\n        if optimizer.param_groups[0][\"lr\"] < min_lr / factor:\r\n            loops_after_min_lr += 1\r\n\r\n        if loops_after_min_lr == patience_after_min_lr:\r\n            print(f'Converged Badly!')\r\n            return model, epoch + 1, float('inf'), optimizer.param_groups[0][\"lr\"]\r\n\r\n        \"\"\"\r\n        prev_loss = loss.item() if epoch > 0 else float('inf')\r\n        prev_losses.append(loss.item())\r\n        if epoch >= patience:\r\n            avg_loss_change = sum((prev_losses[i] - prev_losses[i - 1]) for i in range(1, patience)) / patience\r\n            if abs(avg_loss_change) < loss_threshold:\r\n                print(f'Converged Badly! Average loss change: {avg_loss_change}')\r\n                break\r\n            prev_losses.pop(0)\r\n        \"\"\"\r\n\r\n    return model, epoch + 1, loss.item(), optimizer.param_groups[0][\"lr\"]\r\n\r\n\r\ndef testing(model, n_samples, input_dim):\r\n    max_value = 1\r\n\r\n    x_test, y_test = vg.generate_sample_data(n_samples, 0, max_value, input_dim)\r\n    x_test = torch.tensor(x_test, dtype=torch.float)\r\n    y_test = torch.tensor(y_test, dtype=torch.float)\r\n    torch.save(model.state_dict(), 'models/siamese_model.pth')\r\n    # Testowanie modelu\r\n    model.to('cpu')\r\n    x_test.to('cpu')\r\n    y_test.to('cpu')\r\n    with torch.no_grad():\r\n        test_outputs = model(x_test)\r\n        print(\"Test outputs:\")\r\n        print(test_outputs.shape)\r\n        for i in range(test_outputs.shape[0]):\r\n            print(test_outputs[i], y_test[i])\r\n\r\n\r\ndef grid_search(input_dim, hidden_dims, learning_rates, num_layers_list, write_column_names=True):\r\n    best_epoch = float('inf')\r\n    best_params = None\r\n\r\n    path = CSV_FILE_PATH  # + \"_id_\" + str(input_dim) + \".csv\"\r\n    with open(path, mode='a', newline='') as csv_file:\r\n        writer = csv.writer(csv_file)\r\n        if write_column_names:\r\n            writer.writerow(['Input Dim', 'Hidden Dim', 'Learning Rate', 'Num Layers', 'Epoch', 'Loss'])\r\n\r\n        for hidden_dim, learning_rate, num_layers in itertools.product(hidden_dims, learning_rates, num_layers_list):\r\n            model, epoch, loss, out_lr = training(input_dim, hidden_dim, learning_rate, num_layers)\r\n            writer.writerow([input_dim, hidden_dim // input_dim, hidden_dim, learning_rate, num_layers, epoch, loss])\r\n            print(\r\n                f'Parameters: Hidden Dim={hidden_dim}, Learning Rate={out_lr}, Num Layers={num_layers}, Epoch={epoch}, Loss={loss}')\r\n\r\n            if epoch < best_epoch:\r\n                best_epoch = epoch\r\n                best_params = {'hidden_dim': hidden_dim, 'learning_rate': learning_rate, 'num_layers': num_layers,\r\n                               'loss': loss}\r\n\r\n    # do not treat the best params as definitive, always consult with csv,\r\n    # sometimes because of early stop mechanisms the best params cause bigger\r\n    # loss then some other parameters\r\n    print(f'Best Parameters: {best_params}, Best Epoch: {best_epoch}')\r\n\r\n    return best_params\r\n\r\n\r\nif __name__ == '__main__':\r\n    input_dims = [10, 15, 20, 25, 35, 50, 60, 70, 80, 90, 100, 125, 150]\r\n    for input_dim in input_dims:\r\n        hidden_dims = [i * input_dim for i in range(2, 16, 2)]\r\n        print(hidden_dims)\r\n        # hidden_dims = [450]\r\n        learning_rates = [0.01]\r\n        num_layers_list = [1]\r\n        for i in range(1):\r\n            print(\"Loop:\", i, \" for id=\", input_dim)\r\n            best_params = grid_search(input_dim, hidden_dims, learning_rates, num_layers_list, False)\r\n
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	
+++ b/main.py	
@@ -1,3 +1,4 @@
+import time
 import torch
 import torch.nn as nn
 import torch.optim as optim
@@ -25,13 +26,13 @@
     return val_loss
 
 
-def training(input_dim, hidden_dim, learning_rate, num_layers, patience=600):
+def training(input_dim, hidden_dim, learning_rate, num_layers, patience=1000):
     n_samples = 32
     max_value = 1
     min_lr = 1e-8
     patience_after_min_lr = 1000
     loops_after_min_lr = 0
-    factor = 0.75
+    factor = 0.8
 
     device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
 
@@ -41,12 +42,16 @@
     scheduler = ReduceLROnPlateau(optimizer, mode="min", patience=patience, factor=factor, verbose=True, min_lr=min_lr)
 
     # Training loop
-    epochs = 25000
+    epochs = 100000
     for epoch in range(epochs):
         x_train, y_train = vg.generate_sample_data(n_samples, 0, max_value, input_dim, False)
+
+
         x_train = torch.tensor(x_train, dtype=torch.float).to(device)
         y_train = torch.tensor(y_train, dtype=torch.float).to(device)
 
+
+
         optimizer.zero_grad()
         outputs = model(x_train)
         loss = criterion(outputs, y_train)
@@ -59,7 +64,7 @@
             print(
                 f'Epoch [{epoch}/{epochs}], Id: {input_dim} Loss: {loss.item()}, lr={optimizer.param_groups[0]["lr"]}')
 
-        if loss.item() < 0.05:
+        if loss.item() < 0.01:
             break
 
         if optimizer.param_groups[0]["lr"] < min_lr / factor:
@@ -94,17 +99,42 @@
     model.to('cpu')
     x_test.to('cpu')
     y_test.to('cpu')
+    sum =0
+    count = 0
+    max_error = 0
+    min_error = 999
+    start_time = time.time()  # Start timing
     with torch.no_grad():
         test_outputs = model(x_test)
-        print("Test outputs:")
-        print(test_outputs.shape)
-        for i in range(test_outputs.shape[0]):
-            print(test_outputs[i], y_test[i])
+        errors = torch.abs(test_outputs / y_test - 1)
+    elapsed_time = time.time() - start_time  # End timing
+    mean_error = torch.mean(errors).item()
+    max_error = torch.max(errors).item()
+    min_error = torch.min(errors).item()
+
+    start_time = time.time()
+    typical_distances = []
+    for pair in x_test:
+        distance = vg.calculate_distance(pair[0], pair[1])
+        typical_distances.append(distance)
+    typical_time = time.time() - start_time
+
+    print(f"Average Error: {mean_error}")
+    print(f"Maximum Error: {max_error}")
+    print(f"Minimum Error: {min_error}")
+    print(f"Time Taken: {elapsed_time} seconds")
+    print(f"Time Taken Using Traditional Methods: {typical_time} seconds")
+
+
+    return mean_error, max_error
+
 
 
 def grid_search(input_dim, hidden_dims, learning_rates, num_layers_list, write_column_names=True):
     best_epoch = float('inf')
     best_params = None
+    test_resuluts = []
+    errors = []
 
     path = CSV_FILE_PATH  # + "_id_" + str(input_dim) + ".csv"
     with open(path, mode='a', newline='') as csv_file:
@@ -114,6 +144,9 @@
 
         for hidden_dim, learning_rate, num_layers in itertools.product(hidden_dims, learning_rates, num_layers_list):
             model, epoch, loss, out_lr = training(input_dim, hidden_dim, learning_rate, num_layers)
+            test_res, error = testing(model, 1000, input_dim)
+            test_resuluts.append(test_res)
+            errors.append(error)
             writer.writerow([input_dim, hidden_dim // input_dim, hidden_dim, learning_rate, num_layers, epoch, loss])
             print(
                 f'Parameters: Hidden Dim={hidden_dim}, Learning Rate={out_lr}, Num Layers={num_layers}, Epoch={epoch}, Loss={loss}')
@@ -127,16 +160,18 @@
     # sometimes because of early stop mechanisms the best params cause bigger
     # loss then some other parameters
     print(f'Best Parameters: {best_params}, Best Epoch: {best_epoch}')
+    print(test_resuluts)
+    print(errors)
 
     return best_params
 
 
 if __name__ == '__main__':
-    input_dims = [10, 15, 20, 25, 35, 50, 60, 70, 80, 90, 100, 125, 150]
+    input_dims = [100]
     for input_dim in input_dims:
-        hidden_dims = [i * input_dim for i in range(2, 16, 2)]
-        print(hidden_dims)
-        # hidden_dims = [450]
+        #hidden_dims = [i * input_dim for i in range(2, 16, 2)]
+        #print(hidden_dims)
+        hidden_dims = [1000]
         learning_rates = [0.01]
         num_layers_list = [1]
         for i in range(1):
