Index: training_and_searching/training.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import glob\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport os\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport optuna\r\nimport matplotlib.colors as mcolors\r\nimport pandas as pd\r\nimport datetime\r\nfrom models.siamese_model import SiameseNetwork\r\nfrom data_generators import vector_generator as vg\r\n\r\nCSV_FILE_PATH = '../saved_results/results.csv'\r\nbmodel = None\r\nboptimizer = None\r\nbepoch = 0\r\nbmin_loss = float('inf')\r\n\r\n\r\n\"\"\"\r\nThe way this code functions has been altered, but it's still essentially a grid search.\r\nNormally it goes over all the possible combinations of the parameters and trains - each such training is called a study.\r\nStudy may consist of many trials, those are the second to last argument of the run_experiments function. Currently it's\r\nmade so that only data from the most successful trial is saved for more representative results.\r\n\r\nIf [checkpoint_dir] is provided each study with the same structure\r\n(ie. same input_dim, hidden_dim, num_siamese_layers, num_shared_layers)\r\nwill load the model from previous study and continue training from there. \r\nThat way you can easily and efficiently check for example the relation \r\nbetween number of epochs and loss tolerance without going through the \r\nsame number of epochs many times over.\r\n\r\n\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLABEL = \"W-1\"\r\n\r\n\r\ndef moving_average(data, window_size):\r\n    \"\"\"Calculate moving average with variable window size at the edges.\"\"\"\r\n    result = np.convolve(data, np.ones(window_size) / window_size, mode='valid')\r\n    start_avg = [np.mean(data[:i + 1]) for i in range(window_size // 2)]\r\n    end_avg = [np.mean(data[-(i + 1):]) for i in range(window_size // 2, 0, -1)]\r\n    return np.concatenate((start_avg, result, end_avg))\r\ndef validate(model, criterion, x_validate, y_validate, datasize, loss_tolerance, string=\"\", metric='euclidean', test_size=1000):\r\n    model.eval()\r\n    start_time = time.time()\r\n\r\n    with torch.no_grad():\r\n        y_pred = model(x_validate, metric)\r\n        output = y_pred / y_validate\r\n        ones = y_validate / y_validate\r\n        loss = criterion(output, ones)\r\n\r\n    elapsed_time = time.time() - start_time\r\n\r\n    y_validate = y_validate.view(-1)\r\n    y_pred = y_pred.view(-1)\r\n\r\n    sorted_indices = torch.argsort(y_validate)\r\n    y_validate_sorted = y_validate[sorted_indices]\r\n    y_pred_sorted = y_pred[sorted_indices]\r\n\r\n    y_validate_sorted_np = y_validate_sorted.cpu().numpy()\r\n    y_pred_sorted_np = y_pred_sorted.cpu().numpy()\r\n\r\n    errors = torch.abs(y_pred - y_validate)\r\n    relative_errors = errors / torch.abs(y_validate)\r\n\r\n    mean_loss = loss.item()\r\n    max_loss = torch.max(errors).item()\r\n\r\n    #plot_validation_results(y_validate_sorted_np, y_pred_sorted_np, model.input_dim, string, mean_loss, max_loss, datasize, loss_tolerance)\r\n\r\n    print_validation_results(mean_loss, max_loss, errors, relative_errors, elapsed_time, x_validate)\r\n\r\n    start_time2 = time.time()\r\n    typical_distances = [vg.calculate_distance(pair[0], pair[1], metric) for pair in x_validate]\r\n    typical_time = time.time() - start_time2\r\n\r\n    validation_results = {\r\n        'mean_loss': mean_loss,\r\n        'calc_times': (elapsed_time, typical_time),\r\n        'training_size': datasize,\r\n        'test_size' : test_size\r\n    }\r\n\r\n    return validation_results\r\ndef plot_validation_results(y_validate_sorted_np, y_pred_sorted_np, input_dim, string, mean_loss, max_loss, datasize, loss_tolerance):\r\n    plt.figure(figsize=(8, 6))\r\n    plt.plot(y_pred_sorted_np, label='Predicted', color='green')\r\n\r\n    window_size = 10\r\n    moving_avg = moving_average(y_pred_sorted_np, window_size)\r\n    plt.plot(moving_avg, label=f'Moving Average ({window_size})', color='orange')\r\n    plt.plot(y_validate_sorted_np, label='Actual', color='blue')\r\n\r\n    plt.xlabel('Index')\r\n    plt.ylabel('Value')\r\n    plt.title(f'Actual vs Predicted {input_dim} : {string}')\r\n    plt.legend()\r\n    plt.grid(True)\r\n\r\n    plt.text(0.05, 0.95, f'Mean loss: {mean_loss:.4f}', transform=plt.gca().transAxes, fontsize=10,\r\n             verticalalignment='top')\r\n    plt.text(0.05, 0.90, f'Max loss: {max_loss:.4f}', transform=plt.gca().transAxes, fontsize=10,\r\n             verticalalignment='top')\r\n    plt.text(0.05, 0.80, f'Datasize: {datasize}', transform=plt.gca().transAxes, fontsize=10,\r\n             verticalalignment='top')\r\n    plt.text(0.05, 0.75, f'Loss Tolerance: {loss_tolerance}', transform=plt.gca().transAxes, fontsize=10,\r\n             verticalalignment='top')\r\n    plt.legend(loc='lower left')\r\n\r\n    plt.show()\r\ndef print_validation_results(mean_loss, max_loss, errors, relative_errors, elapsed_time, x_validate, metric='euclidean'):\r\n    print(\r\n        f\"\\033[33;5;229mMean loss: {mean_loss}, Max loss: {max_loss}, Min loss: {torch.min(errors).item()}, Mean error: {torch.mean(relative_errors).item()}, \\nMax error: {torch.max(relative_errors).item()}, Min error: {torch.min(relative_errors).item()}\\033[0m\")\r\n\r\n    start_time = time.time()\r\n    typical_distances = [vg.calculate_distance(pair[0], pair[1], metric) for pair in x_validate]\r\n    typical_time = time.time() - start_time\r\n    print(\r\n        f\"\\033[33;5;241mTime Taken: {elapsed_time:.4f} seconds || Time Taken Using Traditional Methods: {typical_time:.4f} seconds\\033[0m\")\r\ndef save_model_checkpoint(model, optimizer, epoch, min_loss, path, input_dim, hidden_dim,num_siamese_layers, num_shared_layers):\r\n    state = {\r\n        'model_state_dict': model.state_dict(),\r\n        'optimizer_state_dict': optimizer.state_dict(),\r\n        'epoch': epoch,\r\n        'min_loss': min_loss\r\n    }\r\n    path = f\"{input_dim}_{hidden_dim}_{num_siamese_layers}_{num_shared_layers}\"+path\r\n    torch.save(state, path)\r\ndef load_model_checkpoint(path, model, optimizer, input_dim, hidden_dim,num_siamese_layers, num_shared_layers):\r\n    path = f\"{input_dim}_{hidden_dim}_{num_siamese_layers}_{num_shared_layers}\"+path\r\n    if os.path.isfile(path):\r\n        state = torch.load(path)\r\n        model.load_state_dict(state['model_state_dict'])\r\n        optimizer.load_state_dict(state['optimizer_state_dict'])\r\n        epoch = state['epoch']\r\n        min_loss = state['min_loss']\r\n        print(f\"\\033[95mLoaded checkpoint '{path}' (epoch {epoch}, min_loss {min_loss})\\033[0m\")\r\n        return model, optimizer, epoch, min_loss\r\n    else:\r\n        return model, optimizer, 0, float('inf')\r\ndef train(model, criterion, optimizer, scheduler, epochs, n_samples, data_size,\r\n          loss_tolerance=0.5, device=torch.device('cpu'), print_every=10, start_epoch=0, min_loss=float('inf'),\r\n          metric='euclidean', checkpoint_path=None, input_dim=None, hidden_dim=None,num_siamese_layers=None, num_shared_layers=None, test_size=1000):\r\n    model.to(device)\r\n    criterion.to(device)\r\n    min_error = float('inf')\r\n    full_x_train, full_y_train = vg.generate_sample_data_with_multithreading(data_size, -1, 1, model.input_dim, metric)\r\n    full_x_train = torch.tensor(full_x_train, dtype=torch.float).to(device)\r\n    full_y_train = torch.tensor(full_y_train, dtype=torch.float).to(device)\r\n\r\n    if checkpoint_path:\r\n        model, optimizer, start_epoch, min_loss = load_model_checkpoint(checkpoint_path, model, optimizer, input_dim, hidden_dim,num_siamese_layers, num_shared_layers)\r\n\r\n    model.train()\r\n\r\n    for epoch in range(start_epoch, epochs):\r\n        try:\r\n            indices = torch.randperm(data_size)[:n_samples]\r\n            x_train = full_x_train[indices]\r\n            y_train = full_y_train[indices]\r\n\r\n            optimizer.zero_grad()\r\n\r\n            output = model(x_train, metric)\r\n            output = output / y_train\r\n            ones = y_train / y_train\r\n            loss = criterion(output, ones)\r\n\r\n            loss.backward()\r\n\r\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\r\n\r\n            optimizer.step()\r\n\r\n            if scheduler is not None:\r\n                scheduler.step(loss.item())\r\n\r\n            if epoch % print_every == 0:\r\n                print(\r\n                    f'\\rId: {model.input_dim} Epoch [{epoch}/{epochs}], Loss: {loss.item()}, Lr: {optimizer.param_groups[0][\"lr\"]} Smallest Found Loss: {min_loss}',\r\n                    end=\"\")\r\n\r\n            if loss.item() < loss_tolerance:\r\n                break\r\n\r\n            if loss.item() < min_loss:\r\n                min_loss = loss.item()\r\n                print(\r\n                    f'\\r\\033[92mId: {model.input_dim} Epoch [{epoch}/{epochs}], Loss: {loss.item()}, '\r\n                    f'Lr: {optimizer.param_groups[0][\"lr\"]} Smallest Found Loss: {min_loss} \\033[0m:'\r\n                    , end=\"\")\r\n        except Exception as e:\r\n            print(f\"Error during training at epoch {epoch}: {e}\")\r\n            break\r\n    global bmodel, boptimizer, bepoch, bmin_loss\r\n    if checkpoint_path and min_loss < bmin_loss:\r\n        bmodel= model\r\n        boptimizer = optimizer\r\n        bepoch = epoch\r\n        bmin_loss = min_loss\r\n\r\n\r\n    print();\r\n    #plot_weights_and_biases(model)\r\n    x_validate, y_validate = vg.generate_sample_data_with_multithreading(test_size, -5000, 5000, model.input_dim, metric)\r\n    x_validate = torch.tensor(x_validate, dtype=torch.float).to(device)\r\n    y_validate = torch.tensor(y_validate, dtype=torch.float).to(device)\r\n\r\n    validation_results = validate(model, criterion, x_validate, y_validate, data_size, loss_tolerance, test_size=test_size)\r\n\r\n    return model, epoch + 1, loss.item(), optimizer.param_groups[0][\r\n        \"lr\"], min_loss, full_x_train, full_y_train, validation_results\r\ndef plot_weights_and_biases(model):\r\n    weights_per_layer = []\r\n    biases_per_layer = []\r\n\r\n    # Iterate over the layers in the model\r\n    for name, param in model.named_parameters():\r\n        if 'weight' in name:\r\n            weights_per_layer.append(param.data.cpu().numpy())\r\n        elif 'bias' in name:\r\n            biases_per_layer.append(param.data.cpu().numpy())\r\n\r\n    num_layers = len(weights_per_layer)\r\n\r\n    # Calculate average output weights for each neuron\r\n    avg_weights = []\r\n    for i in range(num_layers):\r\n        weights = weights_per_layer[i]\r\n        # Calculate mean weight for this layer\r\n        layer_mean_weight = np.mean(weights)\r\n        normalized_weights = weights / layer_mean_weight  # Normalize by layer mean\r\n        avg_weights.append(normalized_weights.mean(axis=1))  # Average over input connections\r\n\r\n    plt.figure(figsize=(12, 6))\r\n    for i in range(num_layers):\r\n        avg_weights_layer = avg_weights[i]\r\n        num_neurons = len(avg_weights_layer)\r\n        # Color represents the weight relative to the layer's mean (importance within layer)\r\n        plt.scatter(np.full(num_neurons, i), np.arange(num_neurons), c=avg_weights_layer, cmap='coolwarm', marker='s', s=100, alpha=0.5)\r\n\r\n    plt.colorbar(label='Normalized Weight (Importance within Layer)')\r\n    plt.xlabel('Layer Index')\r\n    plt.ylabel('Neuron Index')\r\n    plt.title('Normalized Average Output Weights Distribution (Relative Importance)')\r\n    plt.grid(True)\r\n    plt.show()\r\ndef objective(trial, num_siamese_layers, num_shared_layers, hidden_dim, input_dim, epochs, loss_tolerance, data_size, n_samples, metric,test_size, mode):\r\n    device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\r\n\r\n    model = SiameseNetwork(input_dim, hidden_dim, num_siamese_layers, num_shared_layers)\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\r\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=200, factor=0.75, min_lr=1e-10, verbose=True)\r\n    criterion = nn.MSELoss()\r\n\r\n    current_time = datetime.datetime.now()\r\n    model, _, _, _, val_loss, _, _, validation_results = train(model, criterion, optimizer, scheduler, epochs=epochs,\r\n                                                               n_samples=n_samples, loss_tolerance=loss_tolerance,\r\n                                                               device=device, data_size=data_size, print_every=100, metric=metric, checkpoint_path=mode, input_dim=input_dim, hidden_dim=hidden_dim,num_siamese_layers=num_siamese_layers, num_shared_layers=num_shared_layers, test_size=test_size)\r\n    training_time_delta = datetime.datetime.now() - current_time\r\n\r\n    # Access minutes using total_seconds and division\r\n    minutes = int(training_time_delta.total_seconds() / 60)\r\n\r\n    # Access remaining seconds (optional)\r\n    seconds = int(training_time_delta.total_seconds() % 60)\r\n\r\n    # Access microseconds (optional, similar to seconds)\r\n    microseconds = training_time_delta.microseconds\r\n\r\n    # Formatting the time string\r\n    training_time_str = f\"{minutes}:{seconds}.{microseconds}\"\r\n    mean_loss = validation_results['mean_loss']\r\n    calc_times = validation_results['calc_times']\r\n    training_size = validation_results['training_size']\r\n    stest_size = validation_results['test_size']\r\n    label = LABEL\r\n\r\n    trial.set_user_attr('mean_loss', mean_loss)\r\n    trial.set_user_attr('calc_times', calc_times)\r\n    trial.set_user_attr('training_size', training_size)\r\n    trial.set_user_attr('label', label)\r\n    trial.set_user_attr('device', str(device))\r\n    trial.set_user_attr('time', current_time.strftime(\"%Y-%m-%d %H:%M\"))\r\n    trial.set_user_attr('training time', training_time_str)\r\n    trial.set_user_attr('metric', metric)\r\n    trial.set_user_attr('test_size', test_size)\r\n\r\n\r\n    return val_loss\r\ndef run_experiments(hidden_dims, num_siamese_layers_values, num_shared_layers_values, input_dims, epochs_values, loss_tolerance_values, data_size_values, n_samples_values, test_size_values, metrics,trials=1, mode=None):\r\n    results = []\r\n    for metric in metrics:\r\n        for num_siamese_layers in num_siamese_layers_values:\r\n            for num_shared_layers in num_shared_layers_values:\r\n                for input_dim in input_dims:\r\n                    for epochs in epochs_values:\r\n                        for loss_tolerance in loss_tolerance_values:\r\n                            for data_size in data_size_values:\r\n                                for n_samples in n_samples_values:\r\n                                    for test_size in test_size_values:\r\n                                        for hidden_dim in hidden_dims:\r\n                                            trial_results = []\r\n                                            if mode:\r\n                                                global bmodel, boptimizer, bepoch, bmin_loss\r\n                                                bmodel = None\r\n                                                boptimizer = None\r\n                                                bepoch = 0\r\n                                                bmin_loss = float('inf')\r\n                                            study = optuna.create_study(direction='minimize')\r\n                                            study.optimize(lambda trial: objective(trial, num_siamese_layers, num_shared_layers, hidden_dim, input_dim, epochs, loss_tolerance, data_size, n_samples, metric,test_size , mode), n_trials=trials)\r\n                                            trial = study.best_trial\r\n                                            min_loss = trial.value\r\n                                            user_attrs = trial.user_attrs\r\n                                            if mode:\r\n                                                save_model_checkpoint(bmodel, boptimizer, bepoch, bmin_loss, mode, input_dim, hidden_dim,num_siamese_layers, num_shared_layers)\r\n                                            trial_results.append((min_loss, user_attrs))\r\n                                            print(\r\n                                                f\"num_siamese_layers: {num_siamese_layers}, num_shared_layers: {num_shared_layers}, input_dim: {input_dim}, hidden_dim: {hidden_dim}, epochs: {epochs}, loss_tolerance: {loss_tolerance}, data_size: {data_size}, n_samples: {n_samples}, min_loss: {min_loss}\")\r\n                                            print(f'\\033[38;5;208m====================================\\033[0m:')\r\n                                            results.append((num_siamese_layers, num_shared_layers, input_dim, epochs, loss_tolerance, data_size, n_samples, trial_results))\r\n\r\n    return results\r\ndef create_custom_colormap(min_value, max_value):\r\n    # Define colors for the colormap\r\n    colors = [(0, 0.5, 0), (0.6, 0.8, 0.2)]  # Dark green to light green\r\n    # Define the levels for the colormap based on the min and max values\r\n    levels = np.linspace(min_value, max_value, 256)\r\n    # Create the colormap\r\n    cmap = mcolors.LinearSegmentedColormap.from_list('custom_colormap', colors, N=len(levels))\r\n    return cmap\r\ndef save_experiment_results(results, hidden_dims):\r\n    # Create a list to store the results\r\n    data = []\r\n\r\n    for (num_siamese_layers, num_shared_layers, input_dim, epochs, loss_tolerance, data_size, n_samples, trials) in results:\r\n        for hidden_dim, (loss, user_attrs) in zip(hidden_dims, trials):\r\n            row = [\r\n                num_siamese_layers,\r\n                num_shared_layers,\r\n                input_dim,\r\n                hidden_dim,\r\n                loss,\r\n                epochs,\r\n                loss_tolerance,\r\n                data_size,\r\n                n_samples,\r\n                user_attrs.get('mean_loss', np.nan),\r\n                user_attrs.get('calc_times', (np.nan, np.nan))[0],\r\n                user_attrs.get('calc_times', (np.nan, np.nan))[1],\r\n                user_attrs.get('training_size', np.nan),\r\n                user_attrs.get('label', \"\"),\r\n                user_attrs.get('device', \"\"),\r\n                user_attrs.get('time', \"\"),\r\n                user_attrs.get('training time', \"\"),\r\n                user_attrs.get('metric', \"\"),\r\n                user_attrs.get('test_size', np.nan)\r\n            ]\r\n            data.append(row)\r\n\r\n    # Create a DataFrame from the data\r\n    df1 = pd.DataFrame(data, columns=[\r\n        'Siamese Layers', 'Shared Layers', 'Input Dim', 'Hidden Dimension', 'Train Loss',\r\n        'Epochs', 'Loss Tolerance', 'Datasize', 'n_samples', 'Test Loss',\r\n        'Calc Time (Model)', 'Calc Time (Traditional)', 'Training Size', 'Label', 'Device', 'Time', 'Training Time', 'metric', 'Test Size'\r\n    ])\r\n\r\n    # Check if results.csv exists\r\n    if os.path.isfile(CSV_FILE_PATH):\r\n        # If it exists, read the existing data\r\n        df = pd.read_csv(CSV_FILE_PATH, index_col=None)\r\n    else:\r\n        # If it doesn't exist, create an empty DataFrame\r\n        df = pd.DataFrame(columns=[\r\n            'Siamese Layers', 'Shared Layers', 'Input Dim', 'Hidden Dimension', 'Train Loss',\r\n            'Epochs', 'Loss Tolerance', 'Datasize', 'n_samples', 'Test Loss',\r\n            'Calc Time (Model)', 'Calc Time (Traditional)', 'Training Size', 'Label', 'Device', 'Time', 'Training Time',\r\n            'metric', 'Test Size'\r\n        ])\r\n\r\n    # Append the new data to the DataFrame\r\n    df = pd.concat([df, df1], ignore_index=True)\r\n\r\n    # Save the DataFrame to the CSV file\r\n    df.to_csv(CSV_FILE_PATH, index=False)\r\n\r\n\r\ndef delete_checkpoint_files(checkpoint_dir):\r\n  \"\"\"\r\n  This function deletes files containing 'checkpoint_dir' in their path.\r\n\r\n  Args:\r\n    checkpoint_dir: The string to search for in file paths.\r\n  \"\"\"\r\n  files_to_delete = glob.glob(f\"**/*{checkpoint_dir}*\", recursive=True)\r\n  if files_to_delete:\r\n    print(f\"Deleting files containing '{checkpoint_dir}':\")\r\n    for file in files_to_delete:\r\n      print(file)\r\n      os.remove(file)\r\n    print(\"Files deleted successfully!\")\r\n  else:\r\n    print(f\"No files found containing '{checkpoint_dir}'.\")\r\n\r\ndef get_device_and_print_info():\r\n  device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\r\n\r\n  color_code = {\r\n      'cuda': \"\\033[92m\",  # Green for CUDA\r\n      'cpu': \"\\033[91m\"    # Red for CPU\r\n  }\r\n\r\n  print(f\"{color_code[device.type]}==================\\nUsing device:{device}\\n==================\\033[0m\")\r\n\r\n  return device\r\n\r\nif __name__ == '__main__':\r\n    device = get_device_and_print_info()\r\n\r\n    # crucial values - these values determine the topology of the network\r\n    hidden_dims = [500]\r\n    num_siamese_layers_values = [2]\r\n    num_shared_layers_values = [5]\r\n    input_dims = [100]\r\n\r\n\r\n    epochs_values = [5000]\r\n    loss_tolerance_values = [0.5,0.1,0.05]\r\n    data_size_values = [20000]\r\n    test_size_values = [100,500,1000,10000]\r\n\r\n    n_samples_values = [64]\r\n\r\n    metrics = ['euclidean']\r\n    checkpoint_dir = \"checkpoints\"\r\n\r\n\r\n\r\n    results = run_experiments(hidden_dims, num_siamese_layers_values, num_shared_layers_values, input_dims,\r\n                              epochs_values, loss_tolerance_values, data_size_values, n_samples_values,test_size_values , metrics, 1, checkpoint_dir)\r\n    save_experiment_results(results, hidden_dims)\r\n\r\n    delete_checkpoint_files(checkpoint_dir)\r\n\r\n\r\n
===================================================================
diff --git a/training_and_searching/training.py b/training_and_searching/training.py
--- a/training_and_searching/training.py	(revision dc57aa85f6fa93579c18ee524fccf6c9938c6c67)
+++ b/training_and_searching/training.py	(date 1729632820612)
@@ -262,7 +262,7 @@
     plt.grid(True)
     plt.show()
 def objective(trial, num_siamese_layers, num_shared_layers, hidden_dim, input_dim, epochs, loss_tolerance, data_size, n_samples, metric,test_size, mode):
-    device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 
     model = SiameseNetwork(input_dim, hidden_dim, num_siamese_layers, num_shared_layers)
     optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)
@@ -419,7 +419,7 @@
     print(f"No files found containing '{checkpoint_dir}'.")
 
 def get_device_and_print_info():
-  device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')
+  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 
   color_code = {
       'cuda': "\033[92m",  # Green for CUDA
@@ -431,31 +431,34 @@
   return device
 
 if __name__ == '__main__':
-    device = get_device_and_print_info()
+
+    for loopdims in range(100,1000,100):
+
+        device = get_device_and_print_info()
 
-    # crucial values - these values determine the topology of the network
-    hidden_dims = [500]
-    num_siamese_layers_values = [2]
-    num_shared_layers_values = [5]
-    input_dims = [100]
+        # crucial values - these values determine the topology of the network
+        hidden_dims = [500]
+        num_siamese_layers_values = [2]
+        num_shared_layers_values = [5]
+        input_dims = [loopdims]
 
 
-    epochs_values = [5000]
-    loss_tolerance_values = [0.5,0.1,0.05]
-    data_size_values = [20000]
-    test_size_values = [100,500,1000,10000]
+        epochs_values = [5000]
+        loss_tolerance_values = [0.5,0.1,0.05]
+        data_size_values = [20000]
+        test_size_values = [100,500,1000,10000]
 
-    n_samples_values = [64]
+        n_samples_values = [64]
 
-    metrics = ['euclidean']
-    checkpoint_dir = "checkpoints"
+        metrics = ['euclidean']
+        checkpoint_dir = "checkpoints"
 
 
 
-    results = run_experiments(hidden_dims, num_siamese_layers_values, num_shared_layers_values, input_dims,
-                              epochs_values, loss_tolerance_values, data_size_values, n_samples_values,test_size_values , metrics, 1, checkpoint_dir)
-    save_experiment_results(results, hidden_dims)
+        results = run_experiments(hidden_dims, num_siamese_layers_values, num_shared_layers_values, input_dims,
+                                  epochs_values, loss_tolerance_values, data_size_values, n_samples_values,test_size_values , metrics, 1, checkpoint_dir)
+        save_experiment_results(results, hidden_dims)
 
-    delete_checkpoint_files(checkpoint_dir)
+        delete_checkpoint_files(checkpoint_dir)
 
 
Index: training_and_searching/training_recurrent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport itertools\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\nimport csv\r\n\r\nfrom models.recurrect_model import LSTMModel\r\nimport data_generators.vector_generator as vg\r\n\r\nCSV_FILE_PATH = '../results/grid_search.csv'\r\n\r\n\r\ndef validate(model, criterion, vector_size):\r\n    model.eval()\r\n    x_validate, y_validate = vg.generate_sample_data_for_recurrent(100, 0, 1, vector_size, True)\r\n    x_validate = torch.tensor(x_validate, dtype=torch.float)\r\n    y_validate = torch.tensor(y_validate, dtype=torch.float)\r\n\r\n    with torch.no_grad():\r\n        y_pred = model(x_validate)\r\n        loss = criterion(y_pred, y_validate)\r\n    for i in range(len(y_validate)):\r\n        print(\"Predicted:\", y_pred[i], \"Actual:\", y_validate[i])\r\n    print(\"Mean loss:\", loss.item())\r\n    print(\"Max loss:\", torch.max(abs(y_pred - y_validate)))\r\n    print(\"Min loss:\", torch.min(abs(y_pred - y_validate)))\r\n    return loss.item()\r\n\r\n\r\ndef train(model, criterion, optimizer, scheduler, epochs, n_samples,\r\n          loss_tolerance=0.5, device=torch.device('cpu')):\r\n    # Transfer components to device\r\n    model.to(device)\r\n    criterion.to(device)\r\n\r\n    # Training loop\r\n    model.train()\r\n    epoch = 0\r\n    loss = 0\r\n    min_loss = float('inf')\r\n    for epoch in range(epochs):\r\n        # Generate training data\r\n        x_train, y_train = vg.generate_sample_data_for_recurrent(n_samples, 0, 1, model.input_dim)\r\n        x_train = torch.tensor(x_train, dtype=torch.float).to(device)\r\n        y_train = torch.tensor(y_train, dtype=torch.float).to(device)\r\n\r\n        # Calculate loss\r\n        optimizer.zero_grad()\r\n        output = model(x_train)\r\n        loss = criterion(output, y_train)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        if scheduler is not None:\r\n            scheduler.step(loss.item())\r\n\r\n        # Print progress\r\n        if epoch % 10 == 0:\r\n            print(f'Id: {model.input_dim}, Hdr: {model.hidden_dim_r}, Hdf: {model.hidden_dim_fc}, Lrn: {model.num_layers_recurrent}, Lfcn: {model.num_layers_fc} Epoch [{epoch}/{epochs}], Loss: {loss.item()}, Lr: {optimizer.param_groups[0][\"lr\"]} Best loss: {min_loss}')\r\n\r\n        # Check if function converged\r\n        if loss.item() < loss_tolerance:\r\n            break\r\n\r\n        if loss.item() < min_loss:\r\n            min_loss = loss.item()\r\n            print(f\"!!! Found new min_loss {min_loss} in epoch {epoch + 1} !!!\")\r\n\r\n    return model, epoch + 1, loss.item(), optimizer.param_groups[0][\"lr\"]\r\n\r\n\r\ndef grid_search(criterion, optimizer_obj, scheduler_obj, hidden_dims_r, hidden_dims_fc, num_recurrent_layers_list, num_fc_layers_list, epochs, n_samples, lr, loss_tolerance, device):\r\n    scheduler, out_lr = None, None\r\n    path = CSV_FILE_PATH\r\n    with open(path, mode='a', newline='') as csv_file:\r\n        writer = csv.writer(csv_file)\r\n\r\n        for hidden_dim, hidden_dim_fc, num_layers_recurrent, num_layers_fc in itertools.product(hidden_dims_r, hidden_dims_fc, num_recurrent_layers_list, num_fc_layers_list):\r\n            model = LSTMModel(input_dim, hidden_dim, hidden_dim_fc=hidden_dim_fc, num_layers_recurrent=num_layers_recurrent, num_layers_fc=num_layers_fc)\r\n            optimizer = optimizer_obj(model.parameters(), lr=lr)\r\n            if scheduler_obj is not None:\r\n                scheduler = scheduler_obj(optimizer, mode=\"min\", patience=300, factor=0.75, verbose=True, min_lr=1e-8)\r\n\r\n            # train model\r\n            try:\r\n                model, epoch, loss, out_lr = train(model, criterion, optimizer, scheduler, epochs, n_samples, loss_tolerance, device)\r\n            except KeyboardInterrupt:\r\n                print('Closing training early...')\r\n            finally:\r\n                torch.save(model.state_dict(), f\"../saved_models/{model.input_dim}_recurrent_{time.time()}.pth\")\r\n\r\n            validate_value = validate(model, criterion, input_dim)\r\n            writer.writerow([input_dim, hidden_dim // input_dim, hidden_dim, hidden_dim_fc, out_lr, num_layers_recurrent, num_layers_fc, epoch, loss, validate_value])\r\n\r\n\r\nif __name__ == '__main__':\r\n    CSV_FILE_PATH = '../results/loss_tolerance.csv'\r\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n    criterion = nn.L1Loss()\r\n    optimizer = optim.Adam\r\n    scheduler = None\r\n\r\n    input_dims = [100]\r\n    for input_dim in input_dims:\r\n        hidden_dims_r = [64]\r\n        hidden_dims_fc = [300]\r\n        learning_rate = 0.0005\r\n        num_recurrent_layers_list = [2]\r\n        num_fc_layers_list = [3]\r\n        for i in range(1):\r\n            print(\"Loop:\", i, \" for id=\", input_dim)\r\n            best_params = grid_search(criterion, optimizer, scheduler, hidden_dims_r, hidden_dims_fc,\r\n                                      num_recurrent_layers_list, num_fc_layers_list,\r\n                                      epochs=5000000, n_samples=64, lr=learning_rate, loss_tolerance=0.0001, device=device)\r\n
===================================================================
diff --git a/training_and_searching/training_recurrent.py b/training_and_searching/training_recurrent.py
--- a/training_and_searching/training_recurrent.py	(revision dc57aa85f6fa93579c18ee524fccf6c9938c6c67)
+++ b/training_and_searching/training_recurrent.py	(date 1730054010023)
@@ -31,7 +31,7 @@
 
 
 def train(model, criterion, optimizer, scheduler, epochs, n_samples,
-          loss_tolerance=0.5, device=torch.device('cpu')):
+          loss_tolerance=0.1, device=torch.device('cuda')):
     # Transfer components to device
     model.to(device)
     criterion.to(device)
@@ -97,21 +97,24 @@
 
 
 if __name__ == '__main__':
-    CSV_FILE_PATH = '../results/loss_tolerance.csv'
-    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
-    criterion = nn.L1Loss()
-    optimizer = optim.Adam
-    scheduler = None
+    for loopdims in range(400,800,50):
+
+        CSV_FILE_PATH = '../saved_results/loss_tolerance5.csv'
+        device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')
+        print("using device:", device)
+        criterion = nn.L1Loss()
+        optimizer = optim.Adam
+        scheduler = None
 
-    input_dims = [100]
-    for input_dim in input_dims:
-        hidden_dims_r = [64]
-        hidden_dims_fc = [300]
-        learning_rate = 0.0005
-        num_recurrent_layers_list = [2]
-        num_fc_layers_list = [3]
-        for i in range(1):
-            print("Loop:", i, " for id=", input_dim)
-            best_params = grid_search(criterion, optimizer, scheduler, hidden_dims_r, hidden_dims_fc,
-                                      num_recurrent_layers_list, num_fc_layers_list,
-                                      epochs=5000000, n_samples=64, lr=learning_rate, loss_tolerance=0.0001, device=device)
+        input_dims = [loopdims]
+        for input_dim in input_dims:
+            hidden_dims_r = [64]
+            hidden_dims_fc = [300]
+            learning_rate = 0.0005
+            num_recurrent_layers_list = [2]
+            num_fc_layers_list = [3]
+            for i in range(1):
+                print("Loop:", i, " for id=", input_dim)
+                best_params = grid_search(criterion, optimizer, scheduler, hidden_dims_r, hidden_dims_fc,
+                                          num_recurrent_layers_list, num_fc_layers_list,
+                                          epochs=5000000, n_samples=64, lr=learning_rate, loss_tolerance=0.1, device=device)
